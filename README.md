# Proxy para usar modelos de Hugging Face bajo el formato de OpenAI

Un proxy que permite usar modelos de Hugging Face con cualquier herramienta que espere la API de OpenAI, como n8n, LangChain, o aplicaciones personalizadas.

## âœ¨ CaracterÃ­sticas

- ğŸ”„ **Formato OpenAI**: Convierte automÃ¡ticamente entre formatos
- ğŸš€ **Streaming**: Soporte para respuestas en tiempo real
- ğŸ¯ **MÃºltiples modelos**: Llama, Mixtral, Qwen, Hermes y mÃ¡s
- ğŸ“Š **Logging**: Registro detallado de todas las peticiones
- âš¡ **Rate limiting**: ProtecciÃ³n contra abuso (100 req/15min)
- ğŸ” **Monitoring**: Endpoints de salud y estadÃ­sticas

## ğŸš€ InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/YonierGM/Hugginface-proxy
cd proxy-hf-openai

# Instalar dependencias
npm install

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tu token de HF
```

## âš™ï¸ ConfiguraciÃ³n

Crear archivo `.env`:

```env
HF_TOKEN=hf_tu_token_aqui
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct:fireworks-ai
PORT=3000
```

## ğŸ® Uso

### Iniciar servidor

```bash
npm start
```

### PeticiÃ³n bÃ¡sica

```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct:fireworks-ai",
    "messages": [
      {"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}
    ]
  }'
```

### Con streaming

```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.1-70b",
    "messages": [{"role": "user", "content": "Explica la IA"}],
    "stream": true
  }'
```

## ğŸ¤– Modelos disponibles

| Alias           | Modelo completo                                     |
| --------------- | --------------------------------------------------- |
| `llama-3.1-8b`  | `meta-llama/Llama-3.1-8B-Instruct:fireworks-ai`     |
| `llama-3.1-70b` | `meta-llama/Llama-3.1-70B-Instruct:fireworks-ai`    |
| `mixtral-8x7b`  | `mistralai/Mixtral-8x7B-Instruct-v0.1:fireworks-ai` |
| `qwen-2.5-72b`  | `Qwen/Qwen2.5-72B-Instruct:fireworks-ai`            |
| `hermes-3`      | `NousResearch/Hermes-3-Llama-3.1-8B:fireworks-ai`   |

Ver todos: `GET /v1/models`

## ğŸ”§ IntegraciÃ³n con n8n

1. Crear nodo **AI Agent**
2. Configurar:
   - **Provider**: OpenAI
   - **Base URL**: `http://localhost:3000`
   - **API Key**: Tu token de Hugging Face
   - **Model**: `llama-3.1-8b` (o cualquier alias)

## ğŸ“Š Endpoints

| Endpoint               | MÃ©todo | DescripciÃ³n                        |
| ---------------------- | ------ | ---------------------------------- |
| `/v1/chat/completions` | POST   | Chat principal (compatible OpenAI) |
| `/v1/models`           | GET    | Lista de modelos disponibles       |
| `/health`              | GET    | Estado del servidor                |
| `/stats`               | GET    | EstadÃ­sticas del proxy             |

## ğŸ” Monitoreo

```bash
# Verificar salud
curl http://localhost:3000/health

# Ver estadÃ­sticas
curl http://localhost:3000/stats

# Logs en tiempo real
tail -f combined.log
```

## ğŸ¤ Casos de uso

- âœ… **n8n workflows** con modelos de HF
- âœ… **LangChain** apps usando HF models
- âœ… **Aplicaciones personalizadas** que esperan OpenAI API
- âœ… **Chatbots** con modelos open source
- âœ… **Prototipado rÃ¡pido** sin costos de OpenAI

## ğŸ“ Licencia

MIT

## ğŸ†˜ Soporte

Si tienes problemas:

1. Revisa logs: `tail -f error.log`
2. Verifica configuraciÃ³n: `GET /health`
3. Consulta modelos: `GET /v1/models`

---

**â­ Si te fue Ãºtil, dale una estrella al repo!**
